{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S1) Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday was memorable, today is precious, tomorrow will be better\n",
      "Spark version = 2.2.0\n"
     ]
    }
   ],
   "source": [
    "println(\"Yesterday was memorable, today is precious, tomorrow will be better\")\n",
    "println(s\"Spark version = ${sc.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependency files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported files... Sat May 19 07:56:25 PDT 2018 "
     ]
    }
   ],
   "source": [
    "import java.io.File\n",
    "import java.util.Calendar\n",
    "\n",
    "import scala.util.{Try, Success, Failure}\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SQLContext, Encoders}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.graphx._\n",
    "import org.graphframes._\n",
    "\n",
    "\n",
    "print(s\"Successfully imported files... ${Calendar.getInstance().getTime()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/43508054/spark-sql-how-to-read-a-tsv-or-csv-file-into-dataframe-and-apply-a-custom-sche?rq=1\n",
    "\n",
    "https://stackoverflow.com/questions/44009455/scala-convert-list-of-dataframe-into-single-dataframe-then-merge-it-with-anothe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully defined helper functions... Sat May 19 07:56:30 PDT 2018 "
     ]
    }
   ],
   "source": [
    "def getType[T: Manifest](t: T): Manifest[T] = manifest[T]\n",
    "\n",
    "def getListOfFiles(parentDirectory: String, subDirectory: String):List[String] = {\n",
    "    val d = new File(parentDirectory + subDirectory)\n",
    "    if (d.exists && d.isDirectory) {\n",
    "        d.listFiles.filter(_.isFile).toList.map{_.toString}\n",
    "    } else {\n",
    "        List.empty[String]\n",
    "    }\n",
    "}\n",
    "\n",
    "def fromTsvToDF(fileNames: List[String], schema: StructType, bFileHasHeader: Boolean=false): org.apache.spark.sql.DataFrame = {\n",
    "    val allDFs: List[org.apache.spark.sql.DataFrame] = fileNames.map{ \n",
    "        f => {\n",
    "            println(s\"Reading file: $f\")\n",
    "            spark.read.format(\"csv\")        \n",
    "                .option(\"header\", bFileHasHeader.toString)\n",
    "                .option(\"delimiter\", \"\\t\")\n",
    "                .schema(schema)            \n",
    "                .load(f)\n",
    "        }\n",
    "    }\n",
    "    println(s\"fromTsvToDF(): numFiles read = ${allDFs.size}\")\n",
    "    if (allDFs.size > 1)\n",
    "        allDFs.reduce( _ union _)\n",
    "    else if (allDFs.size == 1)\n",
    "      allDFs.head\n",
    "    else \n",
    "      spark.createDataFrame(sc.emptyRDD[Row], schema)\n",
    "}\n",
    "\n",
    "print(s\"Successfully defined helper functions... ${Calendar.getInstance().getTime()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S2) To DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly defined data directories. Sat May 19 07:56:35 PDT 2018\n"
     ]
    }
   ],
   "source": [
    "val dataParentDir=\"/Users/thchang/Documents/dev/personal/ml/connect/git/projects/machine-learning/projects/capstone/data/\"\n",
    "//Users/thchang/Documents/dev/git/tensorlfow/tensorflow/tensorflow/examples/tutorials/word2vec/data\n",
    "val dataParentResultDir=\"result\"\n",
    "\n",
    "val dataQueryAspectInputDir=\"ebay-query-aspects/dev/\"\n",
    "val dataQueryAspectResultDir=\"ebay-query-aspects/result/\"\n",
    "\n",
    "val dataQueryCatInputDir=\"ebay-query-cat/dev/\"\n",
    "val dataQueryCatResultDir=\"ebay-query-cat/result/\"\n",
    "\n",
    "println(s\"Successfuly defined data directories. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  models and schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly defined models and encoders. Sat May 19 07:56:38 PDT 2018\n"
     ]
    }
   ],
   "source": [
    "case class DataQueryAspect(query:String, aspect:String, probAGivenQ:Double, probQGiveA:Double)\n",
    "val DataQueryAspectSchema: StructType = Encoders.product[DataQueryAspect].schema\n",
    "\n",
    "case class DataQueryCat(query:String, catId:String, weight:Float, typeOf:String, priceLow:Float, priceHigh:Float)\n",
    "val DataQueryCatSchema: StructType = Encoders.product[DataQueryCat].schema\n",
    "\n",
    "println(s\"Successfuly defined models and encoders. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /Users/thchang/Documents/dev/personal/ml/connect/git/projects/machine-learning/projects/capstone/data/ebay-query-aspects/dev/a.tsv\n",
      "fromTsvToDF(): numFiles read = 1\n",
      "Reading file: /Users/thchang/Documents/dev/personal/ml/connect/git/projects/machine-learning/projects/capstone/data/ebay-query-cat/dev/query-cat-dev.tsv\n",
      "fromTsvToDF(): numFiles read = 1\n",
      "+--------------------+--------------------+-----------+----------+\n",
      "|               query|              aspect|probAGivenQ|probQGiveA|\n",
      "+--------------------+--------------------+-----------+----------+\n",
      "|   climbers wall art|Style##Arts & Cra...|     -2.416|   -12.631|\n",
      "|   climbers wall art|Features##Persona...|     -2.416|   -12.797|\n",
      "|   climbers wall art|Type##Sticker/3D ...|     -2.416|   -11.134|\n",
      "|   climbers wall art|        Color##Black|     -2.416|   -18.746|\n",
      "|clincher wheelset...|Brand##Oval Concepts|     -2.493|    -8.015|\n",
      "+--------------------+--------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val queryAspectFiles = getListOfFiles(dataParentDir, dataQueryAspectInputDir)\n",
    "val queryAspectRaw: org.apache.spark.sql.DataFrame = fromTsvToDF(queryAspectFiles, DataQueryAspectSchema)  //Dataframe is Dataset[Row]\n",
    "\n",
    "val queryCatFiles = getListOfFiles(dataParentDir, dataQueryCatInputDir)\n",
    "val queryCatRaw: org.apache.spark.sql.DataFrame = fromTsvToDF(queryCatFiles, DataQueryCatSchema, bFileHasHeader=true)\n",
    "\n",
    "queryAspectRaw.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               query|\n",
      "+--------------------+\n",
      "|   climbers wall art|\n",
      "|clincher wheelset...|\n",
      "|affinity series c...|\n",
      "|     maelstorm nexus|\n",
      "|               shoes|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val queryDF = queryAspectRaw.select(\"query\").dropDuplicates()\n",
    "queryDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly saved queryAsepctRaw to file. Sat May 19 08:24:04 PDT 2018\n"
     ]
    }
   ],
   "source": [
    "val outputDir = \"/Users/thchang/Documents/dev/personal/ml/connect/git/projects/machine-learning/projects/capstone/data/ebay-query-aspects\"\n",
    "queryAspectRaw.select(\"query\").write.csv(s\"${outputDir}/query.csv\")\n",
    "println(s\"Successfuly saved queryAsepctRaw to file. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input to wordvec [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queryAspectDF:\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               query|              aspect|\n",
      "+--------------------+--------------------+\n",
      "|   climbers wall art|Style##Arts & Cra...|\n",
      "|   climbers wall art|Features##Persona...|\n",
      "|   climbers wall art|Type##Sticker/3D ...|\n",
      "|   climbers wall art|        Color##Black|\n",
      "|clincher wheelset...|Brand##Oval Concepts|\n",
      "|clincher wheelset...|For Bike Type##Un...|\n",
      "|clincher wheelset...|For Bike Type##Ro...|\n",
      "|clincher wheelset...|Brake Compatibili...|\n",
      "|clincher wheelset...|For Bike Type##Cy...|\n",
      "|affinity series c...|Design/Finish##Pl...|\n",
      "|affinity series c...|Type##Fitted Case...|\n",
      "|affinity series c...|        Color##Clear|\n",
      "|affinity series c...|  Bundle Listing##No|\n",
      "|     maelstorm nexus| Color##Multicolored|\n",
      "|     maelstorm nexus|   Set##Alara Reborn|\n",
      "|     maelstorm nexus|      Set##Darksteel|\n",
      "|     maelstorm nexus|   Type##Enchantment|\n",
      "|     maelstorm nexus|For##Collectors &...|\n",
      "|     maelstorm nexus|      Type##Creature|\n",
      "|            iphone 8|           color:red|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "queryAspectRDD:\n",
      "Style##Arts_&_Crafts/Mission_Style climbers Style##Arts_&_Crafts/Mission_Style wall Style##Arts_&_Crafts/Mission_Style art Style##Arts_&_Crafts/Mission_Style\n",
      "Features##Personalized climbers Features##Personalized wall Features##Personalized art Features##Personalized\n",
      "Type##Sticker/3D_DIY_Clock climbers Type##Sticker/3D_DIY_Clock wall Type##Sticker/3D_DIY_Clock art Type##Sticker/3D_DIY_Clock\n",
      "Color##Black climbers Color##Black wall Color##Black art Color##Black\n",
      "Brand##Oval_Concepts clincher Brand##Oval_Concepts wheelset Brand##Oval_Concepts with Brand##Oval_Concepts 10 Brand##Oval_Concepts speed Brand##Oval_Concepts cog Brand##Oval_Concepts\n",
      "For_Bike_Type##Universal clincher For_Bike_Type##Universal wheelset For_Bike_Type##Universal with For_Bike_Type##Universal 10 For_Bike_Type##Universal speed For_Bike_Type##Universal cog For_Bike_Type##Universal\n",
      "For_Bike_Type##Road_Bike_-_Racing clincher For_Bike_Type##Road_Bike_-_Racing wheelset For_Bike_Type##Road_Bike_-_Racing with For_Bike_Type##Road_Bike_-_Racing 10 For_Bike_Type##Road_Bike_-_Racing speed For_Bike_Type##Road_Bike_-_Racing cog For_Bike_Type##Road_Bike_-_Racing\n",
      "Brake_Compatibility##Rim_Brake clincher Brake_Compatibility##Rim_Brake wheelset Brake_Compatibility##Rim_Brake with Brake_Compatibility##Rim_Brake 10 Brake_Compatibility##Rim_Brake speed Brake_Compatibility##Rim_Brake cog Brake_Compatibility##Rim_Brake\n",
      "For_Bike_Type##Cyclocross_Bike clincher For_Bike_Type##Cyclocross_Bike wheelset For_Bike_Type##Cyclocross_Bike with For_Bike_Type##Cyclocross_Bike 10 For_Bike_Type##Cyclocross_Bike speed For_Bike_Type##Cyclocross_Bike cog For_Bike_Type##Cyclocross_Bike\n",
      "Design/Finish##Plain____ affinity Design/Finish##Plain____ series Design/Finish##Plain____ cover Design/Finish##Plain____ iphone Design/Finish##Plain____ 8 Design/Finish##Plain____ plus Design/Finish##Plain____\n",
      "Type##Fitted_Case/Skin affinity Type##Fitted_Case/Skin series Type##Fitted_Case/Skin cover Type##Fitted_Case/Skin iphone Type##Fitted_Case/Skin 8 Type##Fitted_Case/Skin plus Type##Fitted_Case/Skin\n",
      "Color##Clear affinity Color##Clear series Color##Clear cover Color##Clear iphone Color##Clear 8 Color##Clear plus Color##Clear\n",
      "Bundle_Listing##No affinity Bundle_Listing##No series Bundle_Listing##No cover Bundle_Listing##No iphone Bundle_Listing##No 8 Bundle_Listing##No plus Bundle_Listing##No\n",
      "Color##Multicolored maelstorm Color##Multicolored nexus Color##Multicolored\n",
      "Set##Alara_Reborn maelstorm Set##Alara_Reborn nexus Set##Alara_Reborn\n",
      "Set##Darksteel maelstorm Set##Darksteel nexus Set##Darksteel\n",
      "Type##Enchantment maelstorm Type##Enchantment nexus Type##Enchantment\n",
      "For##Collectors_&_Hobbyists maelstorm For##Collectors_&_Hobbyists nexus For##Collectors_&_Hobbyists\n",
      "Type##Creature maelstorm Type##Creature nexus Type##Creature\n",
      "color:red iphone color:red 8 color:red\n",
      "size:16mb iphone size:16mb 8 size:16mb\n",
      "color:red galaxy color:red nexus color:red\n",
      "size:16mb galaxy size:16mb nexus size:16mb\n",
      "color:red shoes color:red\n",
      "size:16mb htc size:16mb\n",
      "file:////Users/thchang/Documents/dev/personal/ml/connect/git/projects/machine-learning/projects/capstone/data/ebay-query-aspects/result//Mon_Apr_02_10:35:57_PDT_2018\n",
      "\n",
      "Successfully converted to DF. Mon Apr 02 10:35:58 PDT 2018\n"
     ]
    }
   ],
   "source": [
    "val queryAspectDF = queryAspectRaw.select(queryAspectRaw(\"query\"), queryAspectRaw(\"aspect\"))\n",
    "println(s\"queryAspectDF:\\n\")\n",
    "queryAspectDF.show\n",
    "\n",
    "val queryAspectRDD: org.apache.spark.rdd.RDD[String] = queryAspectDF.rdd.map { r => {\n",
    "            val queryTokens: Seq[String] = r.get(0).asInstanceOf[String].split(\" \")\n",
    "            val aspect: String = r.get(1).toString.replaceAll(\"\\\\s\", \"_\")\n",
    "            \n",
    "            aspect + \" \" + queryTokens.mkString(s\" $aspect \" ) + \" \" + aspect    \n",
    "        }\n",
    "}\n",
    "println(s\"\\nqueryAspectRDD:\\n${queryAspectRDD.collect.mkString(\"\\n\")}\")\n",
    "\n",
    "val outputDir = s\"file:///$dataParentDir$dataQueryAspectResultDir/${Calendar.getInstance().getTime()}\".replaceAll(\" \", \"_\")\n",
    "println(outputDir)\n",
    "queryAspectRDD.saveAsTextFile(outputDir)\n",
    "\n",
    "println(s\"\\nSuccessfully converted to DF. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar queries via approximation [WIP]\n",
    "Not sure how well this will work because I am picking the first and last queries. Not sure how well this will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarQueriesByAspectsRDD[2]: (iphone 8,WrappedArray(galaxy nexus, htc)), (iphone 8,WrappedArray(galaxy nexus, shoes))\n",
      "\n",
      "similarQueriesByCatRDD[1]: (iphone 8,WrappedArray(galaxy nexus, htc))\n",
      "                                                                                \n",
      "unionRDD=(iphone 8,(WrappedArray(galaxy nexus, htc),WrappedArray(galaxy nexus, htc))), (iphone 8,(WrappedArray(galaxy nexus, shoes),WrappedArray(galaxy nexus, htc)))\n",
      "\n",
      "unionFinal=List(WrappedArray(galaxy nexus, htc), iphone 8)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.functions.{array, collect_list}\n",
    "val cachedQueries = Seq(\"iphone 8\", \"htc\")\n",
    "val cachedQueriesBroadcast = sc.broadcast(cachedQueries)\n",
    "\n",
    "val aspectToQueriesDF = queryAspectRaw.groupBy(\"aspect\").agg(collect_set(\"query\") as \"qAspectList\")\n",
    "//println(s\"\\naspectToQueriesDF:\")\n",
    "//println(s\"${aspectToQueriesDF.show()}\")\n",
    "val similarQueriesByAspectsRDD = aspectToQueriesDF.rdd.flatMap{\n",
    "    row:Row => {\n",
    "        val queries = row.get(1).asInstanceOf[Seq[String]].filter(word => word.nonEmpty && word.charAt(0).isLetter).sorted\n",
    "        if (queries.size >1) {\n",
    "            val queriesOfInterest = cachedQueriesBroadcast.value intersect queries\n",
    "            if (queriesOfInterest.nonEmpty) {\n",
    "                val key = queriesOfInterest.head\n",
    "                val rest = queries.filter(f => !f.equals(key))\n",
    "                Some(key, rest)\n",
    "            } else {                \n",
    "                //Some((queries.head, queries.last))    \n",
    "                Some((queries.head, queries.tail.toSeq))\n",
    "            }        \n",
    "        } else None\n",
    "    }\n",
    "}.distinct.cache\n",
    "println(s\"\\nsimilarQueriesByAspectsRDD[${similarQueriesByAspectsRDD.count}]: ${similarQueriesByAspectsRDD.collect.mkString(\", \")}\")\n",
    "\n",
    "\n",
    "\n",
    "val catToQueriesDF = queryCatRaw.groupBy(\"catId\").agg(collect_list(\"query\") as \"qCatList\")\n",
    "//println(s\"\\ncatToQueriesDF:\")\n",
    "//println(s\"${catToQueriesDF.show()}\")\n",
    "val similarQueriesByCatRDD = catToQueriesDF.rdd.flatMap{\n",
    "    row:Row => {\n",
    "        val queries = row.get(1).asInstanceOf[Seq[String]].filter(word => word.nonEmpty && word.charAt(0).isLetter).sorted\n",
    "        if (queries.size >1) {\n",
    "            val queriesOfInterest = cachedQueriesBroadcast.value intersect queries\n",
    "            if (queriesOfInterest.nonEmpty) {\n",
    "                val key = queriesOfInterest.head\n",
    "                val rest = queries.filter(f => !f.equals(key))\n",
    "                Some(key, rest)\n",
    "            } else {                \n",
    "                //Some((queries.head, queries.last))    \n",
    "                Some((queries.head, queries.tail.toSeq))\n",
    "            }        \n",
    "        } else None\n",
    "    }\n",
    "}.distinct.cache\n",
    "println(s\"\\nsimilarQueriesByCatRDD[${similarQueriesByCatRDD.count}]: ${similarQueriesByCatRDD.collect.mkString(\", \")}\")\n",
    "\n",
    "val unionRDD = similarQueriesByAspectsRDD join similarQueriesByCatRDD\n",
    "println(s\"\\nunionRDD=${unionRDD.collect.mkString(\", \")}\")\n",
    "\n",
    "val unionFinal = unionRDD.flatMap {\n",
    "    case (q, similarQueriesTuples) => {\n",
    "        val queries: List[Any] = similarQueriesTuples.productIterator.toList //convert from N dimension tuple to list\n",
    "        val duplicates = queries.groupBy(identity).collect { case (x, List(_,_,_*)) => x } // duplicates means entry appear in both cat and aspecect list\n",
    "        if (duplicates.nonEmpty) {\n",
    "            Some( duplicates.toSeq :+ q)\n",
    "        } else None\n",
    "        \n",
    "    }\n",
    "}\n",
    "println(s\"\\nunionFinal=${unionFinal.collect.mkString(\", \")}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar Queries - [Works, but EXPENSIVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarQueriesByAspectsRDD[2]: WrappedArray(galaxy nexus, iphone 8), WrappedArray(galaxy nexus, iphone 8, shoes)\n",
      "\n",
      "similarQueriesByCategoryRDD[1]: WrappedArray(.095 echo crossfire trimmer line, galaxy nexus, iphone 8)\n"
     ]
    }
   ],
   "source": [
    "def removeElementFromList(values: Seq[String], removeValue:String): Seq[String] = {\n",
    "    var s : scala.collection.mutable.Set[String] = scala.collection.mutable.Set(values:_* )\n",
    "    s -= removeValue\n",
    "    s.toSeq\n",
    "}\n",
    "\n",
    "//queryAspectRaw.show(4)\n",
    "// Step1: Group aspects to Queries\n",
    "val aspectToQueriesDF = queryAspectRaw.groupBy(\"aspect\").agg(collect_list(\"query\") as \"qList\")\n",
    "//println(\"aspectToQueriesDF:\\n\")\n",
    "//aspectToQueriesDF.show()\n",
    "\n",
    "// Queries are similar if they share one common aspect.\n",
    "// Note: Converting DF to RDD yields a RDD[Row]\n",
    "val similarQueriesByAspectsRDD = aspectToQueriesDF.rdd.flatMap{\n",
    "    row:Row => {\n",
    "        val queries = row.get(1).asInstanceOf[Seq[String]]\n",
    "        queries.map{ q =>\n",
    "            (q, queries.toSet)\n",
    "        }\n",
    "        if (queries.size >1) Some(queries.sorted) else None\n",
    "    }\n",
    "}.cache\n",
    "println(s\"\\nsimilarQueriesByAspectsRDD[${similarQueriesByAspectsRDD.count}]: ${similarQueriesByAspectsRDD.collect.mkString(\", \")}\")\n",
    "\n",
    "\n",
    "val catToQueriesDF = queryCatRaw.groupBy(\"catId\").agg(collect_list(\"query\") as \"qList\")\n",
    "//catToQueriesDF.show()\n",
    "val similarQueriesByCategoryRDD = catToQueriesDF.rdd.flatMap{\n",
    "    row:Row => {\n",
    "        val queries = row.get(1).asInstanceOf[Seq[String]]\n",
    "        if (queries.size>1) Some(queries.sorted) else None\n",
    "    }    \n",
    "}.cache\n",
    "println(s\"\\nsimilarQueriesByCategoryRDD[${similarQueriesByCategoryRDD.count}]: ${similarQueriesByCategoryRDD.collect.mkString(\", \")}\")\n",
    "\n",
    "val finalCandidatesRDD = similarQueriesByAspectsRDD.cartesian(similarQueriesByCategoryRDD).filter{\n",
    "    case(a:Seq[String], b:Seq[String]) => {\n",
    "        //println(s\"\\t a=${a.mkString(\",\")}  b=${b.mkString(\",\")}\")\n",
    "        a.nonEmpty && b.nonEmpty && ((a intersect b).size > 1)\n",
    "    }\n",
    "}.map { \n",
    "    case(a:Seq[String], b:Seq[String]) => { a intersect b } \n",
    "}.distinct\n",
    "\n",
    "println(s\"\\nfinalCandidatesRDD= ${finalCandidatesRDD.collect().mkString(\", \")}\")\n",
    "\n",
    "println(s\"\\nFind Similar Queries. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputRDD: (maths,50)\n",
      "(maths,60)\n",
      "(english,65)\n",
      "\n",
      "mappedRDD: (maths,(50,1))\n",
      "(maths,(60,1))\n",
      "(english,(65,1))\n",
      "\n",
      "reducedRDD: (english,(65,1))\n",
      "(maths,(110,2))\n",
      "\n",
      "averageRDD: (english,65)\n",
      "(maths,55)\n",
      "Successfully defined WIP functions. ${Calendar.getInstance().getTime()}"
     ]
    }
   ],
   "source": [
    "val inputRDD = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "println(s\"\\ninputRDD: ${inputRDD.collect.mkString(\"\\n\")}\")\n",
    "\n",
    "val mappedRDD = inputRDD.mapValues(mark => (mark, 1));\n",
    "println(s\"\\nmappedRDD: ${mappedRDD.collect.mkString(\"\\n\")}\")\n",
    "\n",
    "val reducedRDD = mappedRDD.reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))\n",
    "println(s\"\\nreducedRDD: ${reducedRDD.collect.mkString(\"\\n\")}\")\n",
    "\n",
    "val averageRDD = reducedRDD.map { x =>\n",
    "    val temp = x._2\n",
    "    val total = temp._1\n",
    "    val count = temp._2\n",
    "    (x._1, total / count)\n",
    "}\n",
    "println(s\"\\naverageRDD: ${averageRDD.collect.mkString(\"\\n\")}\")\n",
    "\n",
    "print(\"Successfully defined WIP functions. ${Calendar.getInstance().getTime()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*\n",
    "val qA1 = queryAspectDF.select(queryAspectDF(\"query\"), queryAspectDF(\"aspect\"))\n",
    "qA1.show\n",
    "\n",
    "val qA2 = qA1.groupBy(\"query\").agg(collect_list(\"aspect\") as \"list\")\n",
    "\n",
    "\n",
    "val qA3RDD = qA2.rdd.flatMap{ \n",
    "    r => {\n",
    "        val queryTokens: Seq[String] = r.get(0).asInstanceOf[String].split(\" \")\n",
    "        val aspects: Seq[String] = r.get(1).asInstanceOf[Seq[String]].map{_.replaceAll(\"\\\\s\", \"_\")}\n",
    "\n",
    "        aspects.map { a =>\n",
    "            a + \" \" + queryTokens.mkString(s\" $a \") + \" \" + a\n",
    "        }\n",
    "        //(queryTokens, aspects)\n",
    "    }\n",
    "}\n",
    "println(s\"qA3RDD: ${qA3RDD.count} \\n${qA3RDD.collect.mkString(\"\\n\")}\\n\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert RELATIONSHIPS to DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (S3) Tensor flow program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
